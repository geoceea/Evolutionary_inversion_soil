# Import necessary libraries
import glob
import numpy as np
import matplotlib.pyplot as plt
import matplotlib as mpl

import pandas as pd
import verde as vd
import multiprocessing
from itertools import accumulate

from tqdm import tqdm

import xarray as xr

import time, array, random

from deap import base, creator, tools, algorithms

from CODES.modeling import create_seismic_model,calculate_parameters,calculate_parameters_from_vs
from CODES.dispersion_curves import create_velocity_model_from_profile,create_velocity_model_from_profile_vs,estimate_disp_from_velocity_model

from parameters_py.config import (
					MODEL_NAME,FOLDER_OUTPUT,
                    MIN_THICK_LAYER,MAX_THICK_LAYER,MAX_TOTAL,MAX_LAYERS,LOW_VELS,UP_VELS,
                    MUTPB,CXPB,HOF_NUM,NGEN,POPULATION,N_INV,
                    NUM_PROCESS)





np.random.seed(42)  # any integer works
random.seed(42)  # any integer works




















data_path = FOLDER_OUTPUT+MODEL_NAME+'/DATA/'





figures_path = FOLDER_OUTPUT+MODEL_NAME+'/FIGURES/'








filename_feather = data_path+'observed_data.feather'

obs_data = pd.read_feather(filename_feather)





obs_data





def create_layers(min_thick_layer, max_thick_layer,max_total=MAX_TOTAL,max_layers=MAX_LAYERS):

    '''
    This function generates a list of random layer thicknesses that sum exactly to a specified total (`max_total`). 
    Each layer thickness is randomly drawn from a uniform distribution between `min_thick_layer` and `max_thick_layer`, 
    ensuring that all layers meet the minimum thickness requirement (`min_thick_layer`). 
    The final layer is adjusted so that the total thickness precisely matches `max_total`.

    Parameters:
    -----------
    min_thick_layer : float
        Minimum thickness of an individual layer (m).
    max_thick_layer : float
        Maximum thickness of an individual layer  (m).
    max_total : float, optional (default=2 meters)
        Maximum total thickness of all layers (m).
    max_layers : float, optional (default=5 layers)
        Number of layers of the model.

    Returns:
    --------
    thick_lst : list of float
        A list of layer thicknesses.

    '''

    values = []
    current_sum = 0.0
    
    while len(values) < max_layers:
        remaining = round(max_total - current_sum, 2)
        
        if remaining <= 0:
            break
        
        possible_values = [v for v in np.arange(min_thick_layer, max_thick_layer+0.01, 0.01) if v <= remaining]
        
        if not possible_values or len(values) == max_layers - 1:
            values.append(remaining)
            break
        
        choice = random.choice(possible_values)
        values.append(round(choice, 2))
        current_sum = round(current_sum + choice, 2)
        
    return values

# ------------------------------------------------------------------

def uniform(low_thick, up_thick,max_total,max_layers,low_vels,up_vels):
    """
    Generates a random velocity model with layer thicknesses (m) and Vs values (m/s).

    This function first creates a set of random layer thicknesses using 
    `create_layers()`, then assigns shear wave velocities (Vs) to each layer 
    based on a uniform distribution.

    Parameters:
    -----------
    low_thick : float
        Lower bound for layer thickness (m).
    up_thick : float
        Upper bound for layer thickness (m).
    low_vels : float
        Lower bound for Vs values (m/s).
    up_vels : float
        Upper bound for Vs values (m/s).
    max_total : float, optional (default=2 meters)
        Maximum total thickness of all layers (m).
    max_layers : float, optional (default=5 layers)
        Number of layers of the model.

    Returns:
    --------
    model : list
        A list containing:
        - `thickness_lst` (list of float): Layer thicknesses.
        - `vs_lst` (list of float): Corresponding Vs values.

    Notes:
    ------
    - The first layer's Vs is sampled uniformly between `low_vels` and `up_vels`.
    - Subsequent layers have Vs values increasing with depth, where the upper 
      and lower bounds for Vs are scaled by the layer index.
    - This function is used in the DEAP framework for generating models:
      
        toolbox.register("model", uniform, lower_thick, upper_thick, lower_vs, upper_vs)
    """
    
    thickness_lst = create_layers(low_thick,up_thick,max_total,max_layers)
    
    vs_lst = []
    for s in range(1,len(thickness_lst)+1):
        if s == 1:
            vs_lst.append(round(np.random.uniform(low_vels,up_vels)))
        else:
            vs_lst.append(round(np.random.uniform(low_vels*(0.75*s),up_vels*(0.5*s))))
    return [thickness_lst,vs_lst]








def inversion_objective(individual, true_disp,number_samples=100):
    """
    Objective function for inversion using DEAP.

    This function evaluates the misfit between the experimental Rayleigh wave 
    dispersion data and the theoretical dispersion curve simulated from a given 
    shear wave velocity (Vs) profile.

    Parameters:
    -----------
    individual : list or array
        Estimated Vs profile used for optimization.
    true_disp : array
        Experimental Rayleigh wave phase velocity dispersion data.
    number_samples : int, optional (default=100)
        Number of frequency samples considered for misfit calculation.

    Returns:
    --------
    misfit : float
        Misfit value computed as the root mean square error (RMSE) 
        normalized by the standard deviation of the experimental data.
    
    Notes:
    ------
    - The theoretical dispersion curve is obtained by first creating a velocity 
      model from the Vs profile and then estimating the Rayleigh wave phase velocities.
    - The misfit formula is:
    
        misfit = sqrt(sum((xdi - xci)^2 / σi^2) / nf)
        
      where:
        - xdi: Experimental Rayleigh wave phase velocity at frequency fi
        - xci: Theoretical Rayleigh wave phase velocity for the trial model at fi
        - σi: Standard deviation of the experimental data at fi
        - nf: Number of frequency samples
    - If an error occurs, the function returns a high misfit value (10).
    """
    
    try:
        simulated_velocity_model = create_velocity_model_from_profile_vs(individual)
            
        simulated_cpr = estimate_disp_from_velocity_model(simulated_velocity_model)
            
        simulated_dispersion = simulated_cpr.velocity*1000
                
        nf = number_samples 
        sigma = np.std(true_disp)
    
        misfit = np.sqrt(np.sum(((true_disp - simulated_dispersion) ** 2) / (sigma ** 2)) / nf)

   
    except:
        misfit = 10
        
    return misfit,





def statistics_save(individual):
    """
    Retrieves the fitness value of an individual.

    This function returns the fitness value(s) of the given individual, 
    which is used for tracking statistics such as mean, standard deviation, 
    minimum, and maximum fitness during the optimization process.

    Parameters:
    -----------
    individual : object
        An individual solution with an assigned fitness value.

    Returns:
    --------
    fitness_value : tuple
        The fitness value(s) of the individual.
    """
    
    return individual.fitness.values











def mutate_gaussian(ind, mutpb=MUTPB):
    """
    Applies Gaussian mutation to the individual's layer thicknesses and velocities,
    ensuring the structure of each sublist remains unchanged.

    Parameters:
    -----------
    individual : list
        The individual consisting of two sublists: thicknesses and velocities.
    mutpb : float, optional (default=0.1)
        Probability of mutating each value.

    Returns:
    --------
    tuple
        The mutated individual.
    """
    for i in range(len(ind)):  # Iterate over sublists (thicknesses and velocities)
        for j in range(len(ind[i])):  # Iterate over elements in sublist
            if random.random() < mutpb:  # Mutation probability check
                value = ind[i][j]
                sigma = 0.1 * abs(value)  # Standard deviation as 10% of the current value
                ind[i][j] += round(np.random.normal(0, sigma),2)  # Apply Gaussian noise
    return ind,





def crossover_two_point(ind1, ind2, cxpb=CXPB):
    """
    Applies Two-Point Crossover to individuals while preserving the internal 
    structure of their sublists and respecting the shortest length between them.

    Parameters:
    -----------
    ind1 : list
        The first individual, consisting of two sublists.
    ind2 : list
        The second individual, also consisting of two sublists.
    cxpb : float, optional (default=0.5)
        The probability of performing crossover.

    Returns:
    --------
    tuple
        The two individuals after crossover.
    """
    for i in range(len(ind1)):  # Iterate over sublists (thickness and velocity)
        if random.random() < cxpb:  # Check if crossover occurs for this sublist
            # Determine the shortest length between corresponding sublists
            size = min(len(ind1[i]), len(ind2[i]))
            
            # Select two crossover points
            point1, point2 = sorted(random.sample(range(size), 2))
                
            # Swap values between the two points
            for j in range(point1, point2 + 1):
                ind1[i][j], ind2[i][j] = ind2[i][j], ind1[i][j]
                    
    return ind1, ind2














def configure_deap(estimated_disp,lower_thick,upper_thick,lower_vs,upper_vs,max_total,max_layers):

    # Fitness and Individual Creation:
    if not hasattr(creator, "FitnessMin"):
        creator.create("FitnessMin", base.Fitness, weights=(-1.0,))
    if not hasattr(creator, "Individual"):
        creator.create("Individual", list, fitness=creator.FitnessMin)

    # Toolbox Initialization:
    toolbox = base.Toolbox()

    # Using Multiple Processors 
    pool = multiprocessing.Pool(processes=NUM_PROCESS)
    toolbox.register("map", pool.map)
    
    # Attribute Generator:
    toolbox.register("model", uniform, lower_thick, upper_thick,max_total,max_layers,lower_vs, upper_vs)

    # Individual and Population Initialization:
    toolbox.register("individual", tools.initIterate, creator.Individual, toolbox.model)
    toolbox.register("population", tools.initRepeat, list, toolbox.individual)

    # Evaluation Function:
    toolbox.register("evaluate", inversion_objective, true_disp=estimated_disp)

    # Crossover Operation:
    toolbox.register("mate", crossover_two_point)

    # Mutation Operation:
    toolbox.register("mutate", mutate_gaussian)
    
    # Selection Strategy:
    toolbox.register("select", tools.selTournament, tournsize=10)
    
    return toolbox











dic_inversion = []
start_time = time.time()

# Number of inversions:
n_inv = N_INV

for ivx in tqdm(range(n_inv),total=len(range(n_inv)),desc='Number of inversions', leave=False, dynamic_ncols=True,colour='green'):
    for idx, dat in enumerate(tqdm(obs_data.iterrows(),total=obs_data.shape[0], desc='Station', colour='red',leave=False, dynamic_ncols=True)):
                    
            # Loading the dispersion curves estimated:
            estimated_disp = dat[1]['dispersion_curve']
            
            # Starting DEAP:
            # Parameters (Space search):
            # - Observed dispersion curve
            # - Layers minimum thickness
            # - Layers maximum thickness
            # - First layer minumum velocity
            # - First layer maximum velocity
            # - Maximum thickness
            toolbox = configure_deap(estimated_disp, MIN_THICK_LAYER, MAX_THICK_LAYER, LOW_VELS, UP_VELS,MAX_TOTAL,MAX_LAYERS)
                    
            # Starting population:
            population = toolbox.population(n=POPULATION)

            # Starting statistical measurement:
            estatistica = tools.Statistics(statistics_save)
            estatistica.register('mean', np.mean)
            estatistica.register('min', np.min)
            estatistica.register('max', np.max)

            # Starting best solution estimation:
            hof = tools.HallOfFame(HOF_NUM)
    
            # Starting the algorithm
            result, log = algorithms.eaSimple(population, toolbox, cxpb=CXPB, mutpb=MUTPB, ngen=NGEN,stats=estatistica, halloffame=hof, verbose=False)
            
            # -----------------------------------------------------------------------------------
            # Saving inversion results 
        
            dic_results = {'Vs': [i[1] for i in hof],'thick': [i[0] for i in hof],'misfit': [i['mean'] for i in log],'ngen': [i['gen'] for i in log],'misfit_min':[i['mean'] for i in log],'station':idx+1,'receptor':obs_data['receptor'][idx],'inversion':ivx+1}
            dic_inversion.append(dic_results)
                
# -----------------------------------------------------------------------------------
# End
end_time = time.time()
elapsed_time = end_time - start_time
print('Time spent (m):', elapsed_time / 60)





df_inversion = pd.DataFrame.from_dict(dic_inversion)


df_inversion.to_feather(data_path+'inversion.feather')


df_inversion





df_inversion = pd.read_feather(data_path+'inversion.feather')


fig, ax = plt.subplots(figsize=(8, 6))

# cria um colormap (10 cores, normalizando de 1 a 10)
norm = plt.Normalize(vmin=df_inversion['station'].values.min(), vmax=df_inversion['station'].values.max())
# Norm para associar cada estação a uma cor
norm = mpl.colors.BoundaryNorm(boundaries=np.arange(df_inversion['station'].values.min()-0.5, 
                                                    df_inversion['station'].values.max()+1.5, 1),
                                                    ncolors=len(df_inversion['station'].values))
# forma moderna de pegar colormap
cmap = plt.get_cmap('magma', len(df_inversion['station'].values))  # ainda funciona para cores discretas

for dativ in df_inversion.iterrows():
    color = cmap(norm(dativ[1]['station']))
    ax.semilogy(dativ[1]['ngen'], dativ[1]['misfit'], '-',color=color)
        
ax.set_xlabel('Number of generations')
ax.set_ylabel('Misfit value')
ax.axvline(x=len(dativ[1]['ngen'])-1, color='red', linestyle='--', label='Last gen')
ax.legend(loc='upper right')
ax.grid(True, which='both')
plt.tight_layout()
ax.tick_params(axis='both', which='both', direction='in', 
               top=True, bottom=True, left=True, right=True,
               labelbottom=True, labeltop=False, labelleft=True,
               labelright=True)
ax.grid(which='major', color='gray', linestyle='-', linewidth=0.5)
ax.grid(which='minor', color='gray', linestyle=':', linewidth=0.5, alpha=0.5)
ax.set_title('Misfit evolution')

# ScalarMappable para a colorbar
sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)
sm.set_array([])

# Colorbar discreta
cbar = plt.colorbar(sm, ax=ax, ticks=df_inversion['station'].values)
cbar.set_label("Station")
      
fig.savefig(figures_path+'misfit_total_.png', dpi=300)






